"""
å…‰é‡ä»Šæ—¥å›½æœæ’ä»¶ - AstrBotç‰ˆæœ¬
ç§»æ¤è‡ª nonebot_plugin_sky çš„ "/ä»Šæ—¥å›½æœ" åŠŸèƒ½

æ•°æ®æºï¼šå¾®åš@ä»Šå¤©æ¸¸ç¦»ç¿»è½¦äº†å—, å¾®åš@é™ˆé™ˆåŠªåŠ›ä¸é¸½
ç‰ˆæœ¬ï¼š1.3.0
"""

import asyncio
import re
from datetime import datetime, timedelta, timezone, time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from pathlib import Path
import httpx
import base64
import os
import tempfile
import shutil
import json

from astrbot.api.event import filter, AstrMessageEvent, MessageEventResult, MessageChain
from astrbot.api.star import Context, Star, register
from astrbot.api import logger, AstrBotConfig
import astrbot.api.message_components as Comp


# ==================== å¾®åšçˆ¬è™«ç›¸å…³ç±» ====================

class SpiderException(Exception):
    """çˆ¬è™«åŸºç¡€å¼‚å¸¸"""
    pass


class GetMblogsFailedError(SpiderException):
    """è·å–å¾®åšå¤±è´¥å¼‚å¸¸"""
    pass


class UnknownError(SpiderException):
    """æœªçŸ¥é”™è¯¯å¼‚å¸¸"""
    pass


class Auth:
    """è®¤è¯ä¿¡æ¯ç®¡ç†"""

    # é»˜è®¤æ—¶åŒºï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
    BEIJING_TZ = timezone(timedelta(hours=8))

    def __init__(self, config: Dict = None):
        """åˆå§‹åŒ–è®¤è¯ä¿¡æ¯

        Args:
            config: æ’ä»¶é…ç½®å­—å…¸ï¼ŒåŒ…å«cookiesé…ç½®
        """
        self.config = config or {}
        self.cookies_config = self.config.get("cookies", {})
        self.use_cookie = self.cookies_config.get("enabled", False)
        self._visitor_cookies = {}

    async def init_visitor_auth(self, session: httpx.AsyncClient):
        """åˆå§‹åŒ–è®¿å®¢è®¤è¯ï¼ˆæ— cookieæ–¹æ¡ˆï¼‰"""
        try:
            # Step 1: è·å–SUB
            url = 'https://visitor.passport.weibo.cn/visitor/genvisitor2'
            headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Origin': 'https://visitor.passport.weibo.cn',
                'Referer': 'https://visitor.passport.weibo.cn/visitor/visitor?entry=sinawap&a=enter&url=https%3A%2F%2Fm.weibo.cn%2F',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36'
            }
            data = {'cb': 'visitor_gray_callback', 'tid': '', 'new_tid': 'null'}

            resp = await session.post(url, headers=headers, data=data, timeout=10.0)
            match = re.search(r'visitor_gray_callback\((.*)\)', resp.text)
            if match:
                json_data = json.loads(match.group(1))
                if json_data.get('retcode') == 20000000:
                    sub = json_data['data']['sub']
                    subp = json_data['data']['subp']
                    session.cookies.set('SUB', sub, domain='.weibo.cn')
                    session.cookies.set('SUBP', subp, domain='.weibo.cn')

                    # Step 2: è·å–XSRF-TOKEN
                    resp2 = await session.get('https://m.weibo.cn', headers={'Referer': 'https://visitor.passport.weibo.cn/'}, timeout=10.0)
                    xsrf_token = session.cookies.get('XSRF-TOKEN')
                    self._visitor_cookies = {'xsrf_token': xsrf_token}
                    logger.info("è®¿å®¢è®¤è¯åˆå§‹åŒ–æˆåŠŸ")
                    return True
        except Exception as e:
            logger.error(f"è®¿å®¢è®¤è¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

    def get_headers(self, use_mobile_api: bool = False) -> Dict[str, str]:
        """è·å–æ ‡å‡†è¯·æ±‚å¤´"""
        if use_mobile_api:
            return {
                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "mweibo-pwa": "1",
                "x-requested-with": "XMLHttpRequest",
                "x-xsrf-token": self._visitor_cookies.get('xsrf_token', ''),
                "referer": "https://m.weibo.cn",
            }
        return {
            "user-agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome"
                "/119.0.0.0 Safari/537.36 Edg/119.0.0.0"
            ),
            "cookie": self._get_cookie(),
            "x-xsrf-token": self._get_xsrf_token(),
            "referer": "https://www.weibo.com",
            "sec-ch-ua": '"Microsoft Edge";v="119", "Chromium";v="119", "Not?A_Brand";v="24"',
            "sec-ch-ua-platform": '"Windows"',
        }

    def _get_cookie(self) -> str:
        """è·å–cookie"""
        sub_cookie = self.cookies_config.get("sub", "")
        return f"SUB={sub_cookie}" if sub_cookie else ""

    def _get_xsrf_token(self) -> str:
        """è·å–XSRF Token"""
        return self.cookies_config.get("xsrf_token", "")


@dataclass(frozen=True)
class Urls:
    """å›¾ç‰‡URLé›†åˆ"""
    bmiddle: str
    large: str
    largecover: str
    largest: str
    mw2000: str
    original: str
    thumbnail: str

    def get_preferred_url(self, preference: List[str] = None) -> str:
        """è·å–é¦–é€‰å›¾ç‰‡URL"""
        preference = preference or ["large", "original", "largest"]
        for url_type in preference:
            if url := getattr(self, url_type, None):
                return url
        return self.thumbnail  # æœ€åå…œåº•ä½¿ç”¨ç¼©ç•¥å›¾


@dataclass(frozen=True)
class Picture:
    """å›¾ç‰‡å¯¹è±¡"""
    pic_id: str
    urls: Urls
    size: Optional[Dict[str, int]] = None
    type: str = "jpg"

    def get_url(self, preference: List[str] = None) -> str:
        """è·å–é¦–é€‰å›¾ç‰‡URL"""
        return self.urls.get_preferred_url(preference)

    async def get_binary(self, url: str, client=None, auth: Auth = None):
        """è·å–å›¾ç‰‡çš„äºŒè¿›åˆ¶æ•°æ®"""
        if auth is None:
            auth = Auth()

        async def _get_client():
            if client:
                yield client
            else:
                async with httpx.AsyncClient() as temp_client:
                    yield temp_client

        try:
            async for c in _get_client():
                response = await c.get(url, headers=auth.get_headers(), timeout=10.0)
                response.raise_for_status()
                path = httpx.URL(url).path
                filename = os.path.basename(path)
                return (filename, response.content)

        except httpx.HTTPStatusError as e:
            status = e.response.status_code
            raise GetMblogsFailedError(f"HTTPè¯·æ±‚å¤±è´¥: {status}") from e
        except (httpx.TimeoutException, httpx.NetworkError) as e:
            raise GetMblogsFailedError("ç½‘ç»œè¿æ¥å¼‚å¸¸") from e
        except Exception as e:
            raise UnknownError("å›¾ç‰‡æ•°æ®è·å–å¼‚å¸¸") from e


@dataclass
class Blog:
    """å¾®åšæ¡ç›®"""
    mblogid: str
    text_raw: str
    url: str
    pic_list: List[Picture] = field(default_factory=list)
    created_at: str = ""
    is_long_text: bool = False
    use_mobile_api: bool = False

    # ç¼“å­˜è§£æåçš„æ—¶é—´å¯¹è±¡
    _parsed_datetime: Optional[datetime] = field(
        default=None, compare=False, repr=False
    )

    async def fetch_long_text(self, client: httpx.AsyncClient, max_attempts: int = 3, retry_delay: int = 2, auth: Auth = None) -> str:
        """è·å–é•¿æ–‡æœ¬å†…å®¹ï¼ˆæ”¯æŒé‡è¯•ï¼‰"""
        if not self.is_long_text:
            return self.text_raw

        if auth is None:
            auth = Auth()

        if self.use_mobile_api:
            api = "https://m.weibo.cn/statuses/extend"
            params = {"id": self.mblogid}
        else:
            api = "https://weibo.com/ajax/statuses/longtext"
            params = {"id": self.mblogid}

        for attempt in range(max_attempts):
            try:
                headers = auth.get_headers(use_mobile_api=self.use_mobile_api)
                response = await client.get(api, headers=headers, params=params, timeout=10.0)
                response.raise_for_status()
                content = response.json()

                if content.get("ok") != 1:
                    error_msg = content.get("msg", "æ•°æ®åŠ è½½å¤±è´¥")
                    raise GetMblogsFailedError(f"è·å–é•¿æ–‡æœ¬å¤±è´¥: {error_msg}")

                long_text = content.get("data", ).get("longTextContent", "")
                if long_text:
                    # å°†<br>æ ‡ç­¾è½¬æ¢ä¸ºæ¢è¡Œç¬¦
                    long_text = re.sub(r'<br\s*/?>', '\n', long_text)
                    # æ¸…ç†å…¶ä»–HTMLæ ‡ç­¾
                    return re.sub(r'<[^>]+>', '', long_text).strip()
                return self.text_raw

            except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.NetworkError) as e:
                if attempt < max_attempts - 1:
                    logger.warning(f"è·å–é•¿æ–‡æœ¬å¤±è´¥ (å°è¯• {attempt + 1}/{max_attempts}): {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    logger.warning(f"è·å–é•¿æ–‡æœ¬å¤±è´¥ï¼Œä½¿ç”¨çŸ­æ–‡æœ¬: {e}")
                    return self.text_raw
            except Exception as e:
                logger.warning(f"é•¿æ–‡æœ¬è·å–å¼‚å¸¸ï¼Œä½¿ç”¨çŸ­æ–‡æœ¬: {e}")
                return self.text_raw

        logger.warning(f"é‡è¯•{max_attempts}æ¬¡åä»ç„¶å¤±è´¥ï¼Œä½¿ç”¨çŸ­æ–‡æœ¬")
        return self.text_raw

    async def fetch_images_binary_list(self, auth: Auth = None) -> List[bytes]:
        """è·å–å½“å‰æ–‡ç« å†…æ‰€æœ‰å›¾ç‰‡çš„äºŒè¿›åˆ¶æ•°æ®åˆ—è¡¨"""
        if auth is None:
            auth = Auth()

        tasks = [
            asyncio.create_task(pic.get_binary(pic.get_url(), auth=auth)) for pic in self.pic_list
        ]
        results = await asyncio.gather(*tasks)
        results = [i[1] for i in results if i is not None]
        return results


class Spider:
    """æ–°æµªå¾®åšçˆ¬è™«"""

    _BEIJING_TZ = timezone(timedelta(hours=8))
    _DATE_FORMAT = "%a %b %d %H:%M:%S %z %Y"

    def __init__(self, uid: int, auth: Auth = None):
        """åˆå§‹åŒ–çˆ¬è™«å®ä¾‹"""
        self.uid = uid
        self.auth = auth or Auth()
        self._results: List[Blog] = []
        self._use_mobile_api = False
        self._client: Optional[httpx.AsyncClient] = None
        self._setup_apis()

    def _setup_apis(self) -> None:
        """é…ç½®APIç«¯ç‚¹"""
        self._api = {
            "mblogs": "https://weibo.com/ajax/statuses/mymblog",
            "mobile_mblogs": f"https://m.weibo.cn/api/container/getIndex",
        }

    async def fetch(self, page: int = 0, max_attempts: int = 3, retry_delay: int = 2) -> "Spider":
        """è·å–æŒ‡å®šé¡µç çš„å¾®åšæ•°æ®ï¼ˆæ”¯æŒé‡è¯•å’Œfallbackï¼‰"""
        # ä¼˜å…ˆå°è¯•ç”¨æˆ·é…ç½®çš„cookieæ–¹æ¡ˆ
        if self.auth.use_cookie and self.auth._get_cookie():
            try:
                logger.info("ä½¿ç”¨ç”¨æˆ·é…ç½®çš„cookieè·å–å¾®åšæ•°æ®")
                return await self._fetch_with_cookie(page, max_attempts, retry_delay)
            except Exception as e:
                logger.warning(f"ä½¿ç”¨cookieæ–¹æ¡ˆå¤±è´¥: {e}ï¼Œåˆ‡æ¢åˆ°æ— cookieæ–¹æ¡ˆ")

        # Fallbackåˆ°æ— cookieæ–¹æ¡ˆ
        logger.info("ä½¿ç”¨æ— cookieæ–¹æ¡ˆè·å–å¾®åšæ•°æ®")
        return await self._fetch_without_cookie(page, max_attempts, retry_delay)

    async def _fetch_with_cookie(self, page: int, max_attempts: int, retry_delay: int) -> "Spider":
        """ä½¿ç”¨cookieæ–¹æ¡ˆè·å–æ•°æ®ï¼ˆPCç«¯APIï¼‰"""
        last_exception = None
        headers = self.auth.get_headers(use_mobile_api=False)
        headers["referer"] = f"https://www.weibo.com/u/{self.uid}"

        for attempt in range(max_attempts):
            try:
                params = {"uid": self.uid, "page": page, "feature": 0}
                async with httpx.AsyncClient() as client:
                    response = await client.get(
                        self._api["mblogs"],
                        headers=headers,
                        params=params,
                        timeout=10.0,
                    )
                    response.raise_for_status()
                    content = response.json()

                    if content.get("ok") != 1:
                        error_msg = content.get("msg", "æ•°æ®åŠ è½½å¤±è´¥")
                        raise GetMblogsFailedError(f"è·å–å¾®åšåˆ—è¡¨å¤±è´¥: {error_msg}")
                    if not content.get("data"):
                        raise GetMblogsFailedError("è·å–å¾®åšåˆ—è¡¨å¤±è´¥, è¶…å‡ºæœ€å¤§èƒ½è·å–çš„ç´¢å¼•")

                    self._parse_mblogs(content["data"]["list"])
                    self._use_mobile_api = False
                    return self

            except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.NetworkError) as e:
                last_exception = e
                if attempt < max_attempts - 1:
                    logger.warning(f"è·å–å¾®åšæ•°æ®å¤±è´¥ (å°è¯• {attempt + 1}/{max_attempts}): {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    if isinstance(e, httpx.HTTPStatusError):
                        raise GetMblogsFailedError(f"HTTPè¯·æ±‚å¤±è´¥: {e.response.status_code}") from e
                    else:
                        raise GetMblogsFailedError("ç½‘ç»œè¿æ¥å¼‚å¸¸") from e
            except Exception as e:
                raise UnknownError("æ•°æ®å¤„ç†å¼‚å¸¸") from e

        raise GetMblogsFailedError(f"é‡è¯•{max_attempts}æ¬¡åä»ç„¶å¤±è´¥") from last_exception

    async def _fetch_without_cookie(self, page: int, max_attempts: int, retry_delay: int) -> "Spider":
        """ä½¿ç”¨æ— cookieæ–¹æ¡ˆè·å–æ•°æ®ï¼ˆç§»åŠ¨ç«¯APIï¼‰"""
        last_exception = None

        for attempt in range(max_attempts):
            try:
                self._client = httpx.AsyncClient()
                # åˆå§‹åŒ–è®¿å®¢è®¤è¯
                if not await self.auth.init_visitor_auth(self._client):
                    raise GetMblogsFailedError("è®¿å®¢è®¤è¯åˆå§‹åŒ–å¤±è´¥")

                # ä½¿ç”¨ç§»åŠ¨ç«¯APIè·å–æ•°æ®
                containerid = f"230413{self.uid}"
                params = {"containerid": containerid, "page": page + 1, "count": 10}
                headers = self.auth.get_headers(use_mobile_api=True)
                headers["referer"] = f"https://m.weibo.cn/u/{self.uid}"

                response = await self._client.get(
                    self._api["mobile_mblogs"],
                    headers=headers,
                    params=params,
                    timeout=10.0,
                )
                response.raise_for_status()
                content = response.json()

                if content.get("ok") != 1:
                    error_msg = content.get("msg", "æ•°æ®åŠ è½½å¤±è´¥")
                    raise GetMblogsFailedError(f"è·å–å¾®åšåˆ—è¡¨å¤±è´¥: {error_msg}")

                # è§£æç§»åŠ¨ç«¯APIå“åº”
                self._parse_mobile_mblogs(content.get("data", {}))
                self._use_mobile_api = True
                return self

            except (httpx.HTTPStatusError, httpx.TimeoutException, httpx.NetworkError) as e:
                last_exception = e
                if self._client:
                    await self._client.aclose()
                    self._client = None
                if attempt < max_attempts - 1:
                    logger.warning(f"è·å–å¾®åšæ•°æ®å¤±è´¥ (å°è¯• {attempt + 1}/{max_attempts}): {e}ï¼Œ{retry_delay}ç§’åé‡è¯•...")
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    if isinstance(e, httpx.HTTPStatusError):
                        raise GetMblogsFailedError(f"HTTPè¯·æ±‚å¤±è´¥: {e.response.status_code}") from e
                    else:
                        raise GetMblogsFailedError("ç½‘ç»œè¿æ¥å¼‚å¸¸") from e
            except Exception as e:
                if self._client:
                    await self._client.aclose()
                    self._client = None
                raise UnknownError("æ•°æ®å¤„ç†å¼‚å¸¸") from e

        raise GetMblogsFailedError(f"é‡è¯•{max_attempts}æ¬¡åä»ç„¶å¤±è´¥") from last_exception

    def _parse_mblogs(self, mblogs: List[Dict[str, Any]]) -> None:
        """è§£æå¾®åšæ•°æ®ï¼ˆPCç«¯APIï¼‰"""
        for blog in mblogs:
            self._results.append(
                Blog(
                    mblogid=blog["mblogid"],
                    text_raw=blog["text_raw"],
                    url=f"https://www.weibo.com/{self.uid}/{blog['mblogid']}",
                    created_at=blog["created_at"],
                    pic_list=self._parse_pictures(blog),
                )
            )

    def _parse_mobile_mblogs(self, data: Dict[str, Any]) -> None:
        """è§£æç§»åŠ¨ç«¯APIçš„å¾®åšæ•°æ®"""
        cards = data.get("cards", [])
        for card in cards:
            if card.get("card_type") == 11:
                card_group = card.get("card_group", [])
                for item in card_group:
                    if item.get("card_type") == 9 and "mblog" in item:
                        mblog = item["mblog"]
                        # å°†<br>æ ‡ç­¾è½¬æ¢ä¸ºæ¢è¡Œç¬¦
                        text_raw = re.sub(r'<br\s*/?>', '\n', mblog.get("text", ""))
                        # æ¸…ç†å…¶ä»–HTMLæ ‡ç­¾
                        text_raw = re.sub(r'<[^>]+>', '', text_raw).strip()

                        self._results.append(
                            Blog(
                                mblogid=mblog.get("mid", ""),
                                text_raw=text_raw,
                                url=f"https://m.weibo.cn/status/{mblog.get('mid', '')}",
                                created_at=mblog.get("created_at", ""),
                                pic_list=self._parse_mobile_pictures(mblog),
                                is_long_text=mblog.get("isLongText", False),
                                use_mobile_api=True,
                            )
                        )

    def _parse_pictures(self, blog: Dict[str, Any]) -> List[Picture]:
        """è§£æå¾®åšä¸­çš„å›¾ç‰‡æ•°æ®ï¼ˆPCç«¯APIï¼‰"""
        pic_list = []
        pic_infos = blog.get("pic_infos", {})

        for pic_info in pic_infos.values():
            urls = Urls(
                bmiddle=pic_info.get("bmiddle", {}).get("url", ""),
                large=pic_info.get("large", {}).get("url", ""),
                largecover=pic_info.get("largecover", {}).get("url", ""),
                largest=pic_info.get("largest", {}).get("url", ""),
                mw2000=pic_info.get("mw2000", {}).get("url", ""),
                original=pic_info.get("original", {}).get("url", ""),
                thumbnail=pic_info.get("thumbnail", {}).get("url", ""),
            )
            pic_list.append(Picture(pic_info.get("pic_id"), urls))

        return pic_list

    def _parse_mobile_pictures(self, mblog: Dict[str, Any]) -> List[Picture]:
        """è§£æç§»åŠ¨ç«¯APIçš„å›¾ç‰‡æ•°æ®"""
        pic_list = []
        pics = mblog.get("pics", [])

        for pic in pics:
            # ç§»åŠ¨ç«¯APIçš„å›¾ç‰‡URLç»“æ„
            large_url = pic.get("large", {}).get("url", "")
            urls = Urls(
                bmiddle=pic.get("url", ""),
                large=large_url,
                largecover="",
                largest=large_url,
                mw2000="",
                original=large_url,
                thumbnail=pic.get("url", ""),
            )
            pic_list.append(Picture(pic.get("pid", ""), urls))

        return pic_list

    def filter_by_time(
        self, start: Optional[datetime] = None, end: Optional[datetime] = None
    ) -> "Spider":
        """æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤å¾®åš"""
        # è·å–å½“å‰åŒ—äº¬æ—¶é—´ï¼ˆç”¨äºç¡®å®š"ä»Šå¤©"å’Œé»˜è®¤ç»“æŸæ—¶é—´ï¼‰
        now_beijing = datetime.now(tz=self._BEIJING_TZ)

        if start is None and end is None:
            # æ— å‚æ•° â†’ ä»Šå¤©çš„æ•°æ®
            start, end = self._get_today_range(now_beijing)
        elif start is not None and end is None:
            # å•å‚æ•° â†’ ä»startåˆ°ä»Šå¤©
            start = self._normalize_time(start)
            end = now_beijing
        else:
            # åŒå‚æ•° â†’ æŒ‡å®šèŒƒå›´
            if start is not None:
                start = self._normalize_time(start)
            if end is not None:
                end = self._normalize_time(end)

        # æ‰§è¡Œæ—¶é—´è¿‡æ»¤
        self._results = [
            blog
            for blog in self._results
            if self._is_in_time_range(blog.created_at, start, end)
        ]
        return self

    def _get_today_range(self, now: datetime):
        """è·å–ä»Šå¤©çš„æ—¶é—´èŒƒå›´"""
        today_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
        today_end = now.replace(hour=23, minute=59, second=59, microsecond=999999)
        return today_start, today_end

    def _normalize_time(self, dt: datetime) -> datetime:
        """æ ‡å‡†åŒ–æ—¶é—´å¯¹è±¡ä¸ºåŒ—äº¬æ—¶é—´"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=self._BEIJING_TZ)
        return dt.astimezone(self._BEIJING_TZ)

    def _is_in_time_range(
        self, created_at: str, start: Optional[datetime], end: Optional[datetime]
    ) -> bool:
        """æ£€æŸ¥æ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…"""
        try:
            dt = self._parse_created_at(created_at)

            if start and end:
                return start <= dt <= end
            if start:
                return dt >= start
            if end:
                return dt <= end
            return True

        except (ValueError, TypeError):
            return False

    def _parse_created_at(self, created_at: str) -> datetime:
        """è§£æå¾®åšæ—¶é—´å­—ç¬¦ä¸²"""
        # å¤„ç†å¯èƒ½çš„å¤šä½™ç©ºæ ¼
        clean_time_str = re.sub(r"\s+", " ", created_at).strip()
        dt = datetime.strptime(clean_time_str, self._DATE_FORMAT)
        return dt.astimezone(self._BEIJING_TZ)

    def filter_by_regex(self, pattern: str, flags: int = 0) -> "Spider":
        """æŒ‰æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤å¾®åš"""
        compiled = re.compile(pattern, flags)
        self._results = [
            blog for blog in self._results if compiled.search(blog.text_raw)
        ]
        return self

    def one(self) -> Optional[Blog]:
        """è·å–ç¬¬ä¸€æ¡ç»“æœ"""
        return self._results[0] if self._results else None

    def all(self, limit: Optional[int] = None) -> List[Blog]:
        """è·å–æ‰€æœ‰è¿‡æ»¤ç»“æœ"""
        return self._results[:limit] if limit is not None else self._results.copy()


# ==================== æ ¸å¿ƒæ•°æ®è·å–ç±» ====================

class SkyDaily:
    """å…‰é‡æ¯æ—¥ä»»åŠ¡æ•°æ®è·å–"""

    # ç¼“å­˜é…ç½®
    CACHE_DURATION = 3 * 60 * 60  # 3å°æ—¶ï¼ˆç§’ï¼‰
    CACHE_DIR = "data"

    @staticmethod
    def set_cache_duration(hours: int):
        """è®¾ç½®ç¼“å­˜æ—¶é•¿"""
        SkyDaily.CACHE_DURATION = hours * 60 * 60
        logger.info(f"ç¼“å­˜æ—¶é•¿å·²è®¾ç½®ä¸º: {hours}å°æ—¶")

    @staticmethod
    def _ensure_cache_dir():
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        if not os.path.exists(SkyDaily.CACHE_DIR):
            os.makedirs(SkyDaily.CACHE_DIR, exist_ok=True)

    @staticmethod
    def _load_cache(cache_file_path: str):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        try:
            if os.path.exists(cache_file_path):
                with open(cache_file_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)

                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                cache_time = datetime.fromisoformat(cache_data.get('timestamp', '1970-01-01T00:00:00'))
                current_time = datetime.now()

                if (current_time - cache_time).total_seconds() < SkyDaily.CACHE_DURATION:
                    remaining_hours = (SkyDaily.CACHE_DURATION - (current_time - cache_time).total_seconds()) / 3600
                    logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {cache_file_path}ï¼Œå‰©ä½™: {remaining_hours:.1f}å°æ—¶")
                    return cache_data.get('text'), cache_data.get('images', [])
                else:
                    logger.info(f"ç¼“å­˜å·²è¿‡æœŸ: {cache_file_path}")

        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜ {cache_file_path} å¤±è´¥: {e}")

        return None, None

    @staticmethod
    def _save_cache(text: str, images: List[bytes], cache_file_path: str):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            SkyDaily._ensure_cache_dir()

            # å°†äºŒè¿›åˆ¶å›¾ç‰‡è½¬æ¢ä¸ºbase64å­˜å‚¨
            images_b64 = [base64.b64encode(img).decode('utf-8') for img in images]

            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'text': text,
                'images': images_b64
            }

            with open(cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)

            cache_hours = SkyDaily.CACHE_DURATION / 3600
            logger.info(f"æ•°æ®å·²ç¼“å­˜è‡³ {cache_file_path}ï¼Œæœ‰æ•ˆæœŸ{cache_hours:.1f}å°æ—¶")

        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜ {cache_file_path} å¤±è´¥: {e}")

    @staticmethod
    def _decode_cached_images(images_b64: List[str]) -> List[bytes]:
        """è§£ç ç¼“å­˜ä¸­çš„base64å›¾ç‰‡"""
        try:
            return [base64.b64decode(img) for img in images_b64]
        except Exception as e:
            logger.error(f"è§£ç ç¼“å­˜å›¾ç‰‡å¤±è´¥: {e}")
            return []

    @classmethod
    async def get_daily_data(
        cls,
        uid: int,
        pattern: str,
        author_name: str,
        cache_file_name: str,
        use_cache: bool = True,
        max_attempts: int = 3,
        retry_delay: int = 2,
        auth: Auth = None
    ):
        """
        è·å–æŒ‡å®šå¾®åšæºçš„æ¯æ—¥ä»»åŠ¡ä¿¡æ¯ï¼ˆé€šç”¨æ–¹æ³•ï¼Œæ”¯æŒé‡è¯•ï¼‰

        Args:
            uid (int): å¾®åšç”¨æˆ·ID
            pattern (str): å¾®åšå†…å®¹åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼
            author_name (str): ä½œè€…åï¼ˆç”¨äºç‰ˆæƒä¿¡æ¯ï¼‰
            cache_file_name (str): ç¼“å­˜æ–‡ä»¶åï¼ˆä¾‹å¦‚ "cache_youli.json"ï¼‰
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            max_attempts (int): æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay (int): é‡è¯•é—´éš”ï¼ˆç§’ï¼‰
        """
        cache_file_path = os.path.join(cls.CACHE_DIR, cache_file_name)

        # 1. å°è¯•ä»ç¼“å­˜åŠ è½½
        if use_cache:
            cached_text, cached_images_b64 = cls._load_cache(cache_file_path)
            if cached_text is not None:
                images = cls._decode_cached_images(cached_images_b64)
                return cached_text, images

        # 2. ç¼“å­˜æœªå‘½ä¸­æˆ–ç¦ç”¨ï¼Œä»ç½‘ç»œè·å–
        if use_cache:
            logger.info(f"ç¼“å­˜ {cache_file_name} æœªå‘½ä¸­ï¼Œä»å¾®åš(UID: {uid})è·å–...")
        else:
            logger.info(f"ç¼“å­˜å·²ç¦ç”¨ï¼Œä»å¾®åš(UID: {uid})è·å–...")

        # 3. æ‰§è¡Œçˆ¬è™«ï¼ˆæ”¯æŒé‡è¯•ï¼‰
        if auth is None:
            auth = Auth()
        spider = Spider(uid, auth)

        try:
            await spider.fetch(max_attempts=max_attempts, retry_delay=retry_delay)

            # æ·»åŠ è°ƒè¯•æ—¥å¿—
            all_blogs = spider.filter_by_time().all()
            logger.info(f"è·å–åˆ° {author_name} ä»Šæ—¥å¾®åš {len(all_blogs)} æ¡")

            blog = spider.filter_by_regex(pattern).one()

            if not blog:
                logger.warning(f"æœªæ‰¾åˆ° {author_name}(UID: {uid}) åŒ¹é…æ­£åˆ™ '{pattern}' çš„ä»Šæ—¥ä»»åŠ¡")
                return (f"ã€å›½æœã€‘{author_name} çš„ä»Šæ—¥ä»»åŠ¡è¿˜æœªæ›´æ–°", [])

            # 4. è·å–é•¿æ–‡æœ¬å’Œå›¾ç‰‡ï¼ˆæ”¯æŒé‡è¯•ï¼‰ï¼Œå¤ç”¨spiderçš„client
            if spider._client:
                text = await blog.fetch_long_text(spider._client, max_attempts=max_attempts, retry_delay=retry_delay, auth=auth)
            else:
                text = blog.text_raw
            binary_images = await blog.fetch_images_binary_list(auth=auth)
            if not text:
                text = f"ã€å›½æœã€‘{author_name} çš„ä»Šæ—¥ä»»åŠ¡è¿˜æœªæ›´æ–°"

            final_text = text + cls._generate_copyright(author_name, blog.url)

            # 5. ä¿å­˜åˆ°ç¼“å­˜
            if use_cache:
                cls._save_cache(final_text, binary_images, cache_file_path)

            return (final_text, binary_images)
        finally:
            # ç¡®ä¿å…³é—­client
            if spider._client:
                await spider._client.aclose()

    @classmethod
    def parse_data_source(cls, source_config: str):
        """
        è§£ææ•°æ®æºé…ç½®å­—ç¬¦ä¸²

        Args:
            source_config (str): æ ¼å¼ä¸º "uid:pattern:author_name"

        Returns:
            tuple: (uid, pattern, author_name) æˆ– Noneï¼ˆè§£æå¤±è´¥æ—¶ï¼‰
        """
        try:
            parts = source_config.split(':', 2)  # æœ€å¤šåˆ†å‰²æˆ3éƒ¨åˆ†
            if len(parts) != 3:
                logger.error(f"æ•°æ®æºé…ç½®æ ¼å¼é”™è¯¯: {source_config}ï¼Œåº”ä¸º 'uid:pattern:author_name'")
                return None

            uid_str, pattern, author_name = parts
            uid = int(uid_str.strip())
            pattern = pattern.strip()
            author_name = author_name.strip()

            if not uid or not pattern or not author_name:
                logger.error(f"æ•°æ®æºé…ç½®åŒ…å«ç©ºå€¼: {source_config}")
                return None

            return uid, pattern, author_name

        except ValueError as e:
            logger.error(f"æ•°æ®æºé…ç½®UIDè§£æå¤±è´¥: {source_config}ï¼Œé”™è¯¯: {e}")
            return None
        except Exception as e:
            logger.error(f"æ•°æ®æºé…ç½®è§£æå¼‚å¸¸: {source_config}ï¼Œé”™è¯¯: {e}")
            return None

    @classmethod
    async def get_data_from_sources(cls, data_sources: List[str], use_cache: bool = True, max_attempts: int = 3, retry_delay: int = 2, auth: Auth = None):
        """
        ä»é…ç½®çš„æ•°æ®æºåˆ—è¡¨è·å–æ‰€æœ‰æ•°æ®

        Args:
            data_sources (List[str]): æ•°æ®æºé…ç½®åˆ—è¡¨
            use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            max_attempts (int): æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay (int): é‡è¯•é—´éš”
            auth (Auth): è®¤è¯å®ä¾‹

        Returns:
            List[tuple]: [(text, images, author_name), ...] æˆåŠŸè·å–çš„æ•°æ®åˆ—è¡¨
        """
        results = []

        for i, source_config in enumerate(data_sources):
            try:
                parsed = cls.parse_data_source(source_config)
                if not parsed:
                    continue

                uid, pattern, author_name = parsed
                cache_file_name = f"sky_daily_cache_{uid}.json"

                logger.info(f"æ­£åœ¨è·å–æ•°æ®æº {i+1}/{len(data_sources)}: {author_name} (UID: {uid})")

                text, images = await cls.get_daily_data(
                    uid=uid,
                    pattern=pattern,
                    author_name=author_name,
                    cache_file_name=cache_file_name,
                    use_cache=use_cache,
                    max_attempts=max_attempts,
                    retry_delay=retry_delay,
                    auth=auth
                )

                results.append((text, images, author_name))
                logger.info(f"æˆåŠŸè·å–æ•°æ®æº: {author_name}")

            except Exception as e:
                logger.error(f"è·å–æ•°æ®æºå¤±è´¥: {source_config} ({author_name})ï¼Œé”™è¯¯: {e}", exc_info=True)
                continue

        return results

    @classmethod
    async def get_youli_daily(cls, use_cache: bool = True, auth: Auth = None):
        """è·å– 'ä»Šå¤©æ¸¸ç¦»ç¿»è½¦äº†å—' çš„å›½æœæ¯æ—¥ä»»åŠ¡ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        return await cls.get_daily_data(
            uid=7360748659,
            pattern=r"^#[^#]*å…‰é‡[^#]*è¶…è¯]#\s*\d{1,2}\.\d{1,2}\s*",
            author_name="ä»Šå¤©æ¸¸ç¦»ç¿»è½¦äº†å—",
            cache_file_name="sky_daily_cache_youli.json",
            use_cache=use_cache,
            auth=auth
        )

    @classmethod
    async def get_chenchen_daily(cls, use_cache: bool = True, auth: Auth = None):
        """è·å– 'é™ˆé™ˆåŠªåŠ›ä¸é¸½' çš„å›½æœæ¯æ—¥ä»»åŠ¡ä¿¡æ¯ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        return await cls.get_daily_data(
            uid=5539106873,
            pattern=r"^ã€å›½æœÂ·æ¯æ—¥ä»»åŠ¡æ”»ç•¥ã€‘",
            author_name="é™ˆé™ˆåŠªåŠ›ä¸é¸½",
            cache_file_name="sky_daily_cache_chenchen.json",
            use_cache=use_cache,
            auth=auth
        )

    @staticmethod
    def _generate_copyright(user: str, url: str):
        """ç”Ÿæˆç‰ˆæƒä¿¡æ¯"""
        return "\n------------\n" f"ã€æ•°æ®æ¥æºï¼šå¾®åš@{user}ã€‘\n" f"åŸæ–‡é“¾æ¥ï¼š{url}"


# ==================== AstrBotæ’ä»¶ä¸»ä½“ ====================

@register("sky_daily", "é¡¾ç»¯", "å…‰é‡ä»Šæ—¥å›½æœæ”»ç•¥æŸ¥è¯¢æ’ä»¶", "1.3.0", "https://github.com/Kaguya233qwq/nonebot_plugin_sky")
class SkyDailyPlugin(Star):
    """å…‰é‡ä»Šæ—¥å›½æœæ’ä»¶"""

    def __init__(self, context: Context, config: AstrBotConfig = None):
        super().__init__(context)
        self.config = config or {}
        self.push_task = None  # ç”¨äºå­˜å‚¨æ¨é€ä»»åŠ¡

        # åˆå§‹åŒ–Authå®ä¾‹ï¼Œä¼ é€’é…ç½®
        self.auth = Auth(self.config)

        # åˆå§‹åŒ–ç¼“å­˜é…ç½®
        cache_config = self.config.get("cache", {})
        if cache_config.get("enabled", True):
            cache_duration = cache_config.get("duration", 3)
            SkyDaily.set_cache_duration(cache_duration)
        else:
            logger.info("æ™ºèƒ½ç¼“å­˜å·²ç¦ç”¨")

        # éªŒè¯æ•°æ®æºé…ç½®
        self._validate_data_sources()

        logger.info("å…‰é‡ä»Šæ—¥å›½æœæ’ä»¶å·²åŠ è½½ï¼ˆæ”¯æŒåŠ¨æ€æ•°æ®æºé…ç½®ï¼‰")

        # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨æ¨é€ï¼Œå¯åŠ¨å®šæ—¶ä»»åŠ¡
        if self.config.get("auto_push", {}).get("enabled", False):
            self.push_task = asyncio.create_task(self._daily_push_scheduler())
            logger.info("è‡ªåŠ¨æ¨é€åŠŸèƒ½å·²å¯ç”¨")
        else:
            logger.info("è‡ªåŠ¨æ¨é€åŠŸèƒ½æœªå¯ç”¨")

    def _validate_data_sources(self):
        """éªŒè¯æ•°æ®æºé…ç½®"""
        data_sources = self.config.get("data_sources", [])
        if not data_sources:
            logger.warning("æœªé…ç½®æ•°æ®æºï¼Œå°†ä½¿ç”¨é»˜è®¤æ•°æ®æº")
            return

        valid_count = 0
        for source in data_sources:
            if SkyDaily.parse_data_source(source):
                valid_count += 1

        logger.info(f"æ•°æ®æºé…ç½®éªŒè¯å®Œæˆï¼š{valid_count}/{len(data_sources)} ä¸ªæœ‰æ•ˆ")

    def _get_retry_config(self):
        """è·å–é‡è¯•é…ç½®"""
        retry_config = self.config.get("retry", {})
        return {
            "enabled": retry_config.get("enabled", True),
            "max_attempts": retry_config.get("max_attempts", 3),
            "delay": retry_config.get("delay", 2)
        }

    def _create_forward_message(self, text: str, images: List[bytes], title: str = "å…‰é‡ä»Šæ—¥å›½æœæ”»ç•¥"):
        """åˆ›å»ºåˆå¹¶è½¬å‘æ¶ˆæ¯"""
        # æ„å»ºæ¶ˆæ¯å†…å®¹åˆ—è¡¨
        content = [Comp.Plain(text)]
        temp_files = []

        # å¦‚æœæœ‰å›¾ç‰‡ï¼Œä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶å¹¶æ·»åŠ åˆ°å†…å®¹ä¸­
        if images:
            try:
                # åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„ä¸´æ—¶ç›®å½•
                temp_dir = tempfile.mkdtemp(prefix="sky_daily_")
                for i, image_bytes in enumerate(images):
                    # ä½¿ç”¨.jpgåç¼€ï¼Œå› ä¸ºå¤§å¤šæ•°å¾®åšå›¾ç‰‡æ˜¯jpg
                    temp_file_path = os.path.join(temp_dir, f"image_{i}.jpg")
                    with open(temp_file_path, "wb") as f:
                        f.write(image_bytes)
                    temp_files.append(temp_file_path)
                    content.append(Comp.Image.fromFileSystem(temp_file_path))
            except Exception as e:
                logger.error(f"å¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                # å³ä½¿å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œä¹Ÿå°è¯•å‘é€çº¯æ–‡æœ¬
                content.append(Comp.Plain(f"\n[å›¾ç‰‡åŠ è½½å¤±è´¥: {e}]"))

        # åˆ›å»ºNodeèŠ‚ç‚¹
        node = Comp.Node(
            uin=10001,  # ä½¿ç”¨è™šæ‹ŸID
            name=title,
            content=content
        )

        return node, temp_files

    def _cleanup_temp_files(self, temp_files: List[str]):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•"""
        try:
            temp_dirs = set()
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    temp_dirs.add(os.path.dirname(temp_file))
                    os.remove(temp_file)

            # æ¸…ç†ä¸´æ—¶ç›®å½•
            for temp_dir in temp_dirs:
                if os.path.exists(temp_dir):
                    shutil.rmtree(temp_dir, ignore_errors=True)
            
            if temp_files:
                logger.debug(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {len(temp_files)} ä¸ª")

        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶/ç›®å½•å¤±è´¥: {e}")

    async def _daily_push_scheduler(self):
        """æ¯æ—¥è‡ªåŠ¨æ¨é€è°ƒåº¦å™¨"""
        while True:
            try:
                # è·å–æ¨é€æ—¶é—´é…ç½®
                push_config = self.config.get("auto_push", {})
                push_time_str = push_config.get("push_time", "08:00")
                targets = push_config.get("targets", [])

                if not targets:
                    logger.warning("è‡ªåŠ¨æ¨é€å·²å¯ç”¨ä½†æœªé…ç½®æ¨é€ç›®æ ‡ï¼Œè·³è¿‡æ¨é€")
                    await asyncio.sleep(3600)  # 1å°æ—¶åé‡æ–°æ£€æŸ¥
                    continue

                # è§£ææ¨é€æ—¶é—´
                try:
                    push_hour, push_minute = map(int, push_time_str.split(":"))
                except ValueError:
                    logger.error(f"æ¨é€æ—¶é—´æ ¼å¼é”™è¯¯: {push_time_str}ï¼Œåº”ä¸ºHH:MMæ ¼å¼")
                    await asyncio.sleep(3600)  # 1å°æ—¶åé‡æ–°æ£€æŸ¥
                    continue

                # è·å–å½“å‰æ—¶é—´å’Œç›®æ ‡æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
                now = datetime.now(tz=timezone(timedelta(hours=8)))
                today_push_time = now.replace(hour=push_hour, minute=push_minute, second=0, microsecond=0)

                # å¦‚æœä»Šå¤©çš„æ¨é€æ—¶é—´å·²è¿‡ï¼Œè®¡ç®—æ˜å¤©çš„æ¨é€æ—¶é—´
                if now >= today_push_time:
                    next_push_time = today_push_time + timedelta(days=1)
                else:
                    next_push_time = today_push_time

                # è®¡ç®—ç­‰å¾…æ—¶é—´
                wait_seconds = (next_push_time - now).total_seconds()
                logger.info(f"ä¸‹æ¬¡è‡ªåŠ¨æ¨é€æ—¶é—´: {next_push_time.strftime('%Y-%m-%d %H:%M:%S')} (ç­‰å¾… {wait_seconds:.0f} ç§’)")

                # ç­‰å¾…åˆ°æ¨é€æ—¶é—´
                await asyncio.sleep(wait_seconds)

                # æ‰§è¡Œæ¨é€
                await self._execute_auto_push(targets, push_config)

            except asyncio.CancelledError:
                logger.info("è‡ªåŠ¨æ¨é€ä»»åŠ¡å·²å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(f"è‡ªåŠ¨æ¨é€è°ƒåº¦å™¨å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
                await asyncio.sleep(300)  # 5åˆ†é’Ÿåé‡è¯•

    async def _execute_auto_push(self, targets: List[str], push_config: dict):
        """æ‰§è¡Œè‡ªåŠ¨æ¨é€ï¼ˆæ”¯æŒåŠ¨æ€æ•°æ®æºï¼‰"""
        all_temp_files = []
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œè‡ªåŠ¨æ¨é€ä»Šæ—¥å›½æœæ”»ç•¥åˆ° {len(targets)} ä¸ªç›®æ ‡")

            # è·å–é…ç½®
            retry_config = self._get_retry_config()
            data_sources = self.config.get("data_sources", [])

            # è‡ªåŠ¨æ¨é€å¼ºåˆ¶è·å–æœ€æ–°æ•°æ®ï¼Œä¸ä½¿ç”¨ç¼“å­˜
            logger.info("è‡ªåŠ¨æ¨é€å¼ºåˆ¶è·å–æœ€æ–°æ•°æ®ï¼Œä¸ä½¿ç”¨ç¼“å­˜")

            # è·å–æ•°æ®
            nodes = []
            if not data_sources:
                # ä½¿ç”¨é»˜è®¤æ•°æ®æºï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
                logger.info("è‡ªåŠ¨æ¨é€ä½¿ç”¨é»˜è®¤æ•°æ®æº")
                nodes = await self._get_default_push_nodes(False, all_temp_files)  # ä¸ä½¿ç”¨ç¼“å­˜
            else:
                # ä½¿ç”¨é…ç½®çš„æ•°æ®æº
                logger.info(f"è‡ªåŠ¨æ¨é€ä½¿ç”¨é…ç½®çš„æ•°æ®æºï¼Œå…± {len(data_sources)} ä¸ª")

                if retry_config["enabled"]:
                    results = await SkyDaily.get_data_from_sources(
                        data_sources,
                        use_cache=False,  # ä¸ä½¿ç”¨ç¼“å­˜
                        max_attempts=retry_config["max_attempts"],
                        retry_delay=retry_config["delay"],
                        auth=self.auth
                    )
                else:
                    results = await SkyDaily.get_data_from_sources(
                        data_sources,
                        use_cache=False,  # ä¸ä½¿ç”¨ç¼“å­˜
                        max_attempts=1,
                        retry_delay=0,
                        auth=self.auth
                    )

                # ä¸ºæ¯ä¸ªæ•°æ®æºåˆ›å»ºèŠ‚ç‚¹
                for text, images, author_name in results:
                    try:
                        forward_text = f"ğŸŒ… æ¯æ—¥å…‰é‡å›½æœæ”»ç•¥ (æº: {author_name})\n\n{text}"
                        node, temp_files = self._create_forward_message(
                            forward_text, images, f"ğŸŒ… æ¯æ—¥å…‰é‡æ”»ç•¥({author_name})"
                        )
                        nodes.append(node)
                        all_temp_files.extend(temp_files)
                    except Exception as e:
                        logger.error(f"è‡ªåŠ¨æ¨é€åˆ›å»º[{author_name}]èŠ‚ç‚¹å¤±è´¥: {e}", exc_info=True)

            if not nodes:
                logger.error("è‡ªåŠ¨æ¨é€å¤±è´¥ï¼šæ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥")
                return

            # ä¸ºæ¯ä¸ªç›®æ ‡å‘é€æ¶ˆæ¯
            success_count = 0
            for target in targets:
                target_success = False
                try:
                    for i, node in enumerate(nodes):
                        success = await self.context.send_message(target, MessageChain([node]))
                        if success:
                            logger.info(f"æˆåŠŸæ¨é€æ•°æ®æº{i+1}åˆ°: {target}")
                            target_success = True
                        else:
                            logger.error(f"æ¨é€æ•°æ®æº{i+1}å¤±è´¥: {target}")

                        # ç¨ä½œåœé¡¿ï¼Œé¿å…é£æ§
                        if i < len(nodes) - 1:
                            await asyncio.sleep(1)

                    if target_success:
                        success_count += 1

                except Exception as e:
                    logger.error(f"å‘ {target} æ¨é€å¤±è´¥: {e}", exc_info=True)

            logger.info(f"è‡ªåŠ¨æ¨é€å®Œæˆï¼ŒæˆåŠŸæ¨é€åˆ° {success_count}/{len(targets)} ä¸ªç›®æ ‡")

        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ¨é€æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        finally:
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files(all_temp_files)

    async def _get_default_push_nodes(self, cache_enabled: bool, all_temp_files: List[str]):
        """è·å–é»˜è®¤æ•°æ®æºçš„æ¨é€èŠ‚ç‚¹ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        nodes = []

        # è·å–æ•°æ®æº1 (æ¸¸ç¦») - è‡ªåŠ¨æ¨é€æ—¶å¼ºåˆ¶ä¸ä½¿ç”¨ç¼“å­˜
        try:
            text1, images1 = await SkyDaily.get_youli_daily(use_cache=False, auth=self.auth)
            forward_text1 = f"ğŸŒ… æ¯æ—¥å…‰é‡å›½æœæ”»ç•¥ (æº: æ¸¸ç¦»)\n\n{text1}"
            node1, temp_files1 = self._create_forward_message(forward_text1, images1, "ğŸŒ… æ¯æ—¥å…‰é‡æ”»ç•¥(æ¸¸ç¦»)")
            nodes.append(node1)
            all_temp_files.extend(temp_files1)
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ¨é€è·å–[æ¸¸ç¦»]æ•°æ®å¤±è´¥: {e}", exc_info=True)

        # è·å–æ•°æ®æº2 (é™ˆé™ˆ) - è‡ªåŠ¨æ¨é€æ—¶å¼ºåˆ¶ä¸ä½¿ç”¨ç¼“å­˜
        try:
            text2, images2 = await SkyDaily.get_chenchen_daily(use_cache=False, auth=self.auth)
            forward_text2 = f"ğŸŒ… æ¯æ—¥å…‰é‡å›½æœæ”»ç•¥ (æº: é™ˆé™ˆ)\n\n{text2}"
            node2, temp_files2 = self._create_forward_message(forward_text2, images2, "ğŸŒ… æ¯æ—¥å…‰é‡æ”»ç•¥(é™ˆé™ˆ)")
            nodes.append(node2)
            all_temp_files.extend(temp_files2)
        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ¨é€è·å–[é™ˆé™ˆ]æ•°æ®å¤±è´¥: {e}", exc_info=True)

        return nodes

    @filter.command("ä»Šæ—¥å›½æœ", alias={"sky", "å…‰é‡ä»Šæ—¥å›½æœ", "å›½æœä»Šæ—¥", "ä»Šæ—¥å…‰é‡"})
    async def today_chinese_server(self, event: AstrMessageEvent):
        """è·å–å…‰é‡å›½æœä»Šæ—¥æ”»ç•¥ï¼ˆæ”¯æŒåŠ¨æ€æ•°æ®æºï¼‰"""
        all_temp_files = []  # ç”¨äºè·Ÿè¸ªæ‰€æœ‰ä¸´æ—¶æ–‡ä»¶ï¼Œä¾¿äºæ¸…ç†
        try:
            logger.info(f"ç”¨æˆ· {event.get_sender_name()} æŸ¥è¯¢ä»Šæ—¥å›½æœæ”»ç•¥ [ä¼šè¯: {event.unified_msg_origin}]")

            # è·å–é…ç½®
            cache_enabled = self.config.get("cache", {}).get("enabled", True)
            retry_config = self._get_retry_config()
            data_sources = self.config.get("data_sources", [])

            # å¦‚æœæ²¡æœ‰é…ç½®æ•°æ®æºï¼Œä½¿ç”¨é»˜è®¤æ•°æ®æº
            if not data_sources:
                logger.info("ä½¿ç”¨é»˜è®¤æ•°æ®æº")
                await self._handle_default_sources(event, cache_enabled, all_temp_files)
                return

            # ä½¿ç”¨é…ç½®çš„æ•°æ®æº
            yield event.plain_result(f"æ­£åœ¨è·å–ä»Šæ—¥å›½æœæ”»ç•¥ï¼Œå…± {len(data_sources)} ä¸ªæ•°æ®æº...")

            if retry_config["enabled"]:
                results = await SkyDaily.get_data_from_sources(
                    data_sources,
                    use_cache=cache_enabled,
                    max_attempts=retry_config["max_attempts"],
                    retry_delay=retry_config["delay"],
                    auth=self.auth
                )
            else:
                results = await SkyDaily.get_data_from_sources(
                    data_sources,
                    use_cache=cache_enabled,
                    max_attempts=1,
                    retry_delay=0,
                    auth=self.auth
                )

            if not results:
                yield event.plain_result("æ‰€æœ‰æ•°æ®æºå‡è·å–å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•")
                return

            # å‘é€æ¯ä¸ªæ•°æ®æºçš„ç»“æœ
            for i, (text, images, author_name) in enumerate(results):
                try:
                    node, temp_files = self._create_forward_message(
                        text, images, f"ğŸ“‹ å…‰é‡ä»Šæ—¥å›½æœæ”»ç•¥ (æº: {author_name})"
                    )
                    all_temp_files.extend(temp_files)
                    yield event.chain_result([node])
                    logger.info(f"ä»Šæ—¥å›½æœæ”»ç•¥[{author_name}]å‘é€æˆåŠŸ")
                except Exception as e:
                    logger.error(f"å‘é€[{author_name}]æ•°æ®å¤±è´¥: {e}", exc_info=True)
                    yield event.plain_result(f"å‘é€[{author_name}]æ•°æ®å¤±è´¥ï¼š{e}")

            logger.info(f"ä»Šæ—¥å›½æœæ”»ç•¥æŸ¥è¯¢å®Œæˆï¼ŒæˆåŠŸè·å– {len(results)}/{len(data_sources)} ä¸ªæ•°æ®æº")

        except Exception as e:
            logger.error(f"ä»Šæ—¥å›½æœæ”»ç•¥æŸ¥è¯¢å‡ºç°å¼‚å¸¸: {e}", exc_info=True)
            yield event.plain_result(f"æŸ¥è¯¢æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")

        finally:
            # æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            self._cleanup_temp_files(all_temp_files)

    async def _handle_default_sources(self, event: AstrMessageEvent, cache_enabled: bool, all_temp_files: List[str]):
        """å¤„ç†é»˜è®¤æ•°æ®æºï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰"""
        # 1. è·å–æ•°æ®æº1 (æ¸¸ç¦»)
        yield event.plain_result("æ­£åœ¨è·å–ä»Šæ—¥å›½æœæ”»ç•¥(1/2)... [æº: æ¸¸ç¦»]")
        try:
            text1, images1 = await SkyDaily.get_youli_daily(use_cache=cache_enabled, auth=self.auth)
            node1, temp_files1 = self._create_forward_message(text1, images1, "ğŸ“‹ å…‰é‡ä»Šæ—¥å›½æœæ”»ç•¥ (æº: æ¸¸ç¦»)")
            all_temp_files.extend(temp_files1)
            yield event.chain_result([node1])
            logger.info("ä»Šæ—¥å›½æœæ”»ç•¥[æ¸¸ç¦»]å‘é€æˆåŠŸ")
        except Exception as e:
            logger.error(f"è·å–[æ¸¸ç¦»]æ•°æ®å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"è·å–[æ¸¸ç¦»]æ•°æ®å¤±è´¥ï¼š{e}")

        # 2. è·å–æ•°æ®æº2 (é™ˆé™ˆ)
        yield event.plain_result("æ­£åœ¨è·å–ä»Šæ—¥å›½æœæ”»ç•¥(2/2)... [æº: é™ˆé™ˆ]")
        try:
            text2, images2 = await SkyDaily.get_chenchen_daily(use_cache=cache_enabled, auth=self.auth)
            node2, temp_files2 = self._create_forward_message(text2, images2, "ğŸ“‹ å…‰é‡ä»Šæ—¥å›½æœæ”»ç•¥ (æº: é™ˆé™ˆ)")
            all_temp_files.extend(temp_files2)
            yield event.chain_result([node2])
            logger.info("ä»Šæ—¥å›½æœæ”»ç•¥[é™ˆé™ˆ]å‘é€æˆåŠŸ")
        except Exception as e:
            logger.error(f"è·å–[é™ˆé™ˆ]æ•°æ®å¤±è´¥: {e}", exc_info=True)
            yield event.plain_result(f"è·å–[é™ˆé™ˆ]æ•°æ®å¤±è´¥ï¼š{e}")

    @filter.command("æ¸…é™¤ç¼“å­˜", alias={"æ¸…ç©ºç¼“å­˜", "æ¸…ç†ç¼“å­˜", "skyæ¸…ç¼“å­˜"})
    async def clear_cache(self, event: AstrMessageEvent):
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜çš„ä»»åŠ¡æ•°æ®"""
        try:
            cache_dir = SkyDaily.CACHE_DIR
            if not os.path.exists(cache_dir):
                yield event.plain_result("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
                return

            # æŸ¥æ‰¾æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
            cache_files = []
            for filename in os.listdir(cache_dir):
                if filename.startswith("sky_daily_cache_") and filename.endswith(".json"):
                    cache_files.append(os.path.join(cache_dir, filename))

            if not cache_files:
                yield event.plain_result("æœªæ‰¾åˆ°ä»»ä½•ç¼“å­˜æ–‡ä»¶")
                return

            # åˆ é™¤ç¼“å­˜æ–‡ä»¶
            deleted_count = 0
            for cache_file in cache_files:
                try:
                    os.remove(cache_file)
                    deleted_count += 1
                    logger.info(f"å·²åˆ é™¤ç¼“å­˜æ–‡ä»¶: {cache_file}")
                except Exception as e:
                    logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {cache_file}, é”™è¯¯: {e}")

            yield event.plain_result(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆï¼\nåˆ é™¤äº† {deleted_count} ä¸ªç¼“å­˜æ–‡ä»¶\nä¸‹æ¬¡æŸ¥è¯¢å°†é‡æ–°è·å–æœ€æ–°æ•°æ®")
            logger.info(f"ç”¨æˆ· {event.get_sender_name()} æ¸…é™¤äº† {deleted_count} ä¸ªç¼“å­˜æ–‡ä»¶")

        except Exception as e:
            logger.error(f"æ¸…é™¤ç¼“å­˜æ—¶å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            yield event.plain_result(f"æ¸…é™¤ç¼“å­˜æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")

    @filter.command("å…‰é‡æ¨é€è®¾ç½®", alias={"skyè®¾ç½®", "æ¨é€è®¾ç½®"})
    async def push_settings(self, event: AstrMessageEvent):
        """æ˜¾ç¤ºæ¨é€è®¾ç½®å¸®åŠ©ä¿¡æ¯"""
        data_sources = self.config.get("data_sources", [])
        retry_config = self._get_retry_config()

        help_text = f"""ğŸ”§ å…‰é‡è‡ªåŠ¨æ¨é€è®¾ç½®å¸®åŠ©

ğŸ“ å½“å‰ä¼šè¯æ ‡è¯†: {event.unified_msg_origin}

ğŸ› ï¸ é…ç½®æ­¥éª¤:
1. åœ¨AstrBotç®¡ç†é¢æ¿æ‰¾åˆ°"å…‰é‡ä»Šæ—¥å›½æœæ’ä»¶"é…ç½®
2. é…ç½®ä»¥ä¸‹é€‰é¡¹ï¼š

ğŸ“Š æ•°æ®æºé…ç½®:
- æ ¼å¼: å¾®åšUID:æ­£åˆ™è¡¨è¾¾å¼:ä½œè€…å
- ç¤ºä¾‹: 7360748659:^#[^#]*å…‰é‡[^#]*è¶…è¯]#\\s*\\d{{1,2}}\\.\\d{{1,2}}\\s*:ä»Šå¤©æ¸¸ç¦»ç¿»è½¦äº†å—
- å½“å‰é…ç½®: {len(data_sources)} ä¸ªæ•°æ®æº

ğŸ”„ é‡è¯•æœºåˆ¶é…ç½®:
- å¯ç”¨é‡è¯•: {'æ˜¯' if retry_config['enabled'] else 'å¦'}
- æœ€å¤§é‡è¯•æ¬¡æ•°: {retry_config['max_attempts']}
- é‡è¯•é—´éš”: {retry_config['delay']}ç§’

ğŸ“… è‡ªåŠ¨æ¨é€é…ç½®:
- å¯ç”¨è‡ªåŠ¨æ¨é€: å¼€å¯
- æ¨é€æ—¶é—´: å¦‚ 08:00 (24å°æ—¶åˆ¶)
- æ¨é€ç›®æ ‡åˆ—è¡¨: æ·»åŠ ä¸Šæ–¹çš„ä¼šè¯æ ‡è¯†

ğŸ“‹ ä¼šè¯æ ‡è¯†è¯´æ˜:
- ç¾¤èŠ: platform:GroupMessage:ç¾¤å·
- ç§èŠ: platform:FriendMessage:ç”¨æˆ·ID
- è¯·å¤åˆ¶ä¸Šæ–¹æ˜¾ç¤ºçš„å®Œæ•´æ ‡è¯†ç¬¦

âš ï¸ æ³¨æ„äº‹é¡¹:
- æ¨é€æ—¶é—´ä¸ºåŒ—äº¬æ—¶é—´
- å¯æ·»åŠ å¤šä¸ªæ¨é€ç›®æ ‡å’Œæ•°æ®æº
- ä¿®æ”¹é…ç½®åéœ€é‡è½½æ’ä»¶ç”Ÿæ•ˆ
- æ”¯æŒå¤±è´¥é‡è¯•æœºåˆ¶ï¼Œæé«˜è·å–æˆåŠŸç‡

ğŸ’¡ æ–°åŠŸèƒ½:
- æ”¯æŒè‡ªå®šä¹‰æ•°æ®æºé…ç½®
- ç½‘ç»œè¯·æ±‚å¤±è´¥è‡ªåŠ¨é‡è¯•
- åŠ¨æ€å¾ªç¯è·å–æ‰€æœ‰é…ç½®çš„æ•°æ®æº
- å…¼å®¹æ—§ç‰ˆæœ¬é…ç½®ï¼ˆæœªé…ç½®æ•°æ®æºæ—¶ä½¿ç”¨é»˜è®¤æºï¼‰

ğŸ” æ•°æ®æºé…ç½®è¯´æ˜:
- UID: å¾®åšç”¨æˆ·çš„æ•°å­—ID
- æ­£åˆ™è¡¨è¾¾å¼: ç”¨äºåŒ¹é…å¾®åšå†…å®¹çš„æ¨¡å¼
- ä½œè€…å: æ˜¾ç¤ºåœ¨æ”»ç•¥ä¸­çš„ä½œè€…åç§°
- å¤šä¸ªæ•°æ®æºä¼šä¾æ¬¡è·å–å¹¶å‘é€
"""
        yield event.plain_result(help_text)

    async def terminate(self):
        """æ’ä»¶å¸è½½æ—¶è°ƒç”¨"""
        # å–æ¶ˆè‡ªåŠ¨æ¨é€ä»»åŠ¡
        if self.push_task and not self.push_task.done():
            self.push_task.cancel()
            try:
                await self.push_task
            except asyncio.CancelledError:
                pass
            logger.info("è‡ªåŠ¨æ¨é€ä»»åŠ¡å·²åœæ­¢")

        logger.info("å…‰é‡ä»Šæ—¥å›½æœæ’ä»¶å·²å¸è½½")